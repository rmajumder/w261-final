{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w261 Final Project - Clickthrough Rate Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.functions import when  \n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "app_name = \"w261_final_rishi\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "\n",
    "train_tmp_rdd = sc.textFile('gs://w261hw5rishi/projdata/dac/train.txt')\n",
    "\n",
    "rdd_train = train_tmp_rdd.map(lambda r : r.split('\\t'))\n",
    "\n",
    "df_train = rdd_train.toDF()\n",
    "\n",
    "df_train.write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"gs://w261hw5rishi/projdata/dac/train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = spark.read.csv('gs://w261hw5rishi/projdata/dac/train.csv', header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>_1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>893</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_4</th>\n",
       "      <td>5</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_6</th>\n",
       "      <td>1382</td>\n",
       "      <td>102</td>\n",
       "      <td>767</td>\n",
       "      <td>4392</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_7</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_8</th>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_9</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_10</th>\n",
       "      <td>181</td>\n",
       "      <td>4</td>\n",
       "      <td>245</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_12</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_14</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_15</th>\n",
       "      <td>68fd1e64</td>\n",
       "      <td>68fd1e64</td>\n",
       "      <td>287e684f</td>\n",
       "      <td>68fd1e64</td>\n",
       "      <td>8cf07265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_16</th>\n",
       "      <td>80e26c9b</td>\n",
       "      <td>f0cf0024</td>\n",
       "      <td>0a519c5c</td>\n",
       "      <td>2c16a946</td>\n",
       "      <td>ae46a29d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_17</th>\n",
       "      <td>fb936136</td>\n",
       "      <td>6f67f7e5</td>\n",
       "      <td>02cf9876</td>\n",
       "      <td>a9a87e68</td>\n",
       "      <td>c81688bb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_18</th>\n",
       "      <td>7b4723c4</td>\n",
       "      <td>41274cd7</td>\n",
       "      <td>c18be181</td>\n",
       "      <td>2e17d6f6</td>\n",
       "      <td>f922efad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_19</th>\n",
       "      <td>25c83c98</td>\n",
       "      <td>25c83c98</td>\n",
       "      <td>25c83c98</td>\n",
       "      <td>25c83c98</td>\n",
       "      <td>25c83c98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_20</th>\n",
       "      <td>7e0ccccf</td>\n",
       "      <td>fe6b92e5</td>\n",
       "      <td>7e0ccccf</td>\n",
       "      <td>fe6b92e5</td>\n",
       "      <td>13718bbd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_21</th>\n",
       "      <td>de7995b8</td>\n",
       "      <td>922afcc0</td>\n",
       "      <td>c78204a1</td>\n",
       "      <td>2e8a689b</td>\n",
       "      <td>ad9fa255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_22</th>\n",
       "      <td>1f89b562</td>\n",
       "      <td>0b153874</td>\n",
       "      <td>0b153874</td>\n",
       "      <td>0b153874</td>\n",
       "      <td>0b153874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_23</th>\n",
       "      <td>a73ee510</td>\n",
       "      <td>a73ee510</td>\n",
       "      <td>a73ee510</td>\n",
       "      <td>a73ee510</td>\n",
       "      <td>a73ee510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_24</th>\n",
       "      <td>a8cd5504</td>\n",
       "      <td>2b53e5fb</td>\n",
       "      <td>3b08e48b</td>\n",
       "      <td>efea433b</td>\n",
       "      <td>5282c137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_25</th>\n",
       "      <td>b2cb9c98</td>\n",
       "      <td>4f1b46f3</td>\n",
       "      <td>5f5e6091</td>\n",
       "      <td>e51ddf94</td>\n",
       "      <td>e5d8af57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_26</th>\n",
       "      <td>37c9c164</td>\n",
       "      <td>623049e6</td>\n",
       "      <td>8fe001f4</td>\n",
       "      <td>a30567ca</td>\n",
       "      <td>66a76a26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_27</th>\n",
       "      <td>2824a5f6</td>\n",
       "      <td>d7020589</td>\n",
       "      <td>aa655a2f</td>\n",
       "      <td>3516f6e6</td>\n",
       "      <td>f06c53ac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_28</th>\n",
       "      <td>1adce6ef</td>\n",
       "      <td>b28479f6</td>\n",
       "      <td>07d13a8f</td>\n",
       "      <td>07d13a8f</td>\n",
       "      <td>1adce6ef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_29</th>\n",
       "      <td>8ba8b39a</td>\n",
       "      <td>e6c5b5cd</td>\n",
       "      <td>6dc710ed</td>\n",
       "      <td>18231224</td>\n",
       "      <td>8ff4b403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_30</th>\n",
       "      <td>891b62e7</td>\n",
       "      <td>c92f3b61</td>\n",
       "      <td>36103458</td>\n",
       "      <td>52b8680f</td>\n",
       "      <td>01adbab4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_31</th>\n",
       "      <td>e5ba7672</td>\n",
       "      <td>07c540c4</td>\n",
       "      <td>8efede7f</td>\n",
       "      <td>1e88c74f</td>\n",
       "      <td>1e88c74f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_32</th>\n",
       "      <td>f54016b9</td>\n",
       "      <td>b04e4670</td>\n",
       "      <td>3412118d</td>\n",
       "      <td>74ef3502</td>\n",
       "      <td>26b3c7a7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_33</th>\n",
       "      <td>21ddcdc9</td>\n",
       "      <td>21ddcdc9</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_34</th>\n",
       "      <td>b1252a9d</td>\n",
       "      <td>5840adea</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_35</th>\n",
       "      <td>07b5194c</td>\n",
       "      <td>60f6221e</td>\n",
       "      <td>e587c466</td>\n",
       "      <td>6b3a5ca6</td>\n",
       "      <td>21c9516a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_36</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ad3062eb</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_37</th>\n",
       "      <td>3a171ecb</td>\n",
       "      <td>3a171ecb</td>\n",
       "      <td>3a171ecb</td>\n",
       "      <td>3a171ecb</td>\n",
       "      <td>32c7478e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_38</th>\n",
       "      <td>c5c50484</td>\n",
       "      <td>43f13e8b</td>\n",
       "      <td>3b183c5c</td>\n",
       "      <td>9117a34a</td>\n",
       "      <td>b34f3128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_39</th>\n",
       "      <td>e8b83407</td>\n",
       "      <td>e8b83407</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_40</th>\n",
       "      <td>9727dd16</td>\n",
       "      <td>731c3655</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4\n",
       "_1          0         0         0         0         0\n",
       "_2          1         2         2       NaN         3\n",
       "_3          1         0         0       893        -1\n",
       "_4          5        44         1       NaN       NaN\n",
       "_5          0         1        14       NaN         0\n",
       "_6       1382       102       767      4392         2\n",
       "_7          4         8        89       NaN         0\n",
       "_8         15         2         4         0         3\n",
       "_9          2         2         2         0         0\n",
       "_10       181         4       245         0         0\n",
       "_11         1         1         1       NaN         1\n",
       "_12         2         1         3         0         1\n",
       "_13       NaN       NaN         3       NaN       NaN\n",
       "_14         2         4        45       NaN         0\n",
       "_15  68fd1e64  68fd1e64  287e684f  68fd1e64  8cf07265\n",
       "_16  80e26c9b  f0cf0024  0a519c5c  2c16a946  ae46a29d\n",
       "_17  fb936136  6f67f7e5  02cf9876  a9a87e68  c81688bb\n",
       "_18  7b4723c4  41274cd7  c18be181  2e17d6f6  f922efad\n",
       "_19  25c83c98  25c83c98  25c83c98  25c83c98  25c83c98\n",
       "_20  7e0ccccf  fe6b92e5  7e0ccccf  fe6b92e5  13718bbd\n",
       "_21  de7995b8  922afcc0  c78204a1  2e8a689b  ad9fa255\n",
       "_22  1f89b562  0b153874  0b153874  0b153874  0b153874\n",
       "_23  a73ee510  a73ee510  a73ee510  a73ee510  a73ee510\n",
       "_24  a8cd5504  2b53e5fb  3b08e48b  efea433b  5282c137\n",
       "_25  b2cb9c98  4f1b46f3  5f5e6091  e51ddf94  e5d8af57\n",
       "_26  37c9c164  623049e6  8fe001f4  a30567ca  66a76a26\n",
       "_27  2824a5f6  d7020589  aa655a2f  3516f6e6  f06c53ac\n",
       "_28  1adce6ef  b28479f6  07d13a8f  07d13a8f  1adce6ef\n",
       "_29  8ba8b39a  e6c5b5cd  6dc710ed  18231224  8ff4b403\n",
       "_30  891b62e7  c92f3b61  36103458  52b8680f  01adbab4\n",
       "_31  e5ba7672  07c540c4  8efede7f  1e88c74f  1e88c74f\n",
       "_32  f54016b9  b04e4670  3412118d  74ef3502  26b3c7a7\n",
       "_33  21ddcdc9  21ddcdc9      None      None      None\n",
       "_34  b1252a9d  5840adea      None      None      None\n",
       "_35  07b5194c  60f6221e  e587c466  6b3a5ca6  21c9516a\n",
       "_36      None      None  ad3062eb      None      None\n",
       "_37  3a171ecb  3a171ecb  3a171ecb  3a171ecb  32c7478e\n",
       "_38  c5c50484  43f13e8b  3b183c5c  9117a34a  b34f3128\n",
       "_39  e8b83407  e8b83407      None      None      None\n",
       "_40  9727dd16  731c3655      None      None      None"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df_train.take(5), columns=df_train.columns).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_val(t_data):\n",
    "    #Fill missing values - numerical\n",
    "    return t_data.na.fill(0)\n",
    "\n",
    "\n",
    "def get_columns(t_data):\n",
    "    #Store the original columns\n",
    "    cols = t_data.columns\n",
    "    \n",
    "    #Get numeric and categorical column names\n",
    "    numericCols = t_data.columns[1:14]\n",
    "    categoricalColumns = t_data.columns[16:41]\n",
    "    \n",
    "    return cols, numericCols, categoricalColumns\n",
    "\n",
    "def match_test_cols_with_train_cols(t_data):\n",
    "    \n",
    "    new_names = []\n",
    "    \n",
    "    #rename cols to match train data\n",
    "    for c in t_data.columns:\n",
    "        curr_pos = c.split('_')[1]\n",
    "        new_names.append('_' + str(int(curr_pos) + 1))\n",
    "   \n",
    "    t_data = t_data.toDF(*new_names)\n",
    "    \n",
    "    #include a dummy output col\n",
    "    t_data = t_data.withColumn(\"_1\", lit(0))\n",
    "    \n",
    "    return t_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_string_indexer_vector_assm_pipeline_stages(num_cols, cat_cols):\n",
    "    stages = []\n",
    "    indexerCols = []\n",
    "\n",
    "    for categoricalCol in cat_cols:\n",
    "        indexerCol = categoricalCol + \"Index\"\n",
    "        indexer = StringIndexer(inputCol=categoricalCol, outputCol= indexerCol).setHandleInvalid(\"keep\")\n",
    "        stages += [indexer]\n",
    "        indexerCols.append(indexerCol)\n",
    "\n",
    "    \n",
    "    label_stringIdx = StringIndexer(inputCol = '_1', outputCol = 'output')\n",
    "    stages += [label_stringIdx]\n",
    "    \n",
    "    assembler = VectorAssembler(inputCols=indexerCols + num_cols, outputCol=\"features\")\n",
    "    stages += [assembler]\n",
    "    \n",
    "    return stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_transformation_pipeline_stages(t_data, cols, stages):\n",
    "    pipeline = Pipeline(stages = stages)\n",
    "    pipelineModel = pipeline.fit(t_data)\n",
    "    t_data = pipelineModel.transform(t_data)\n",
    "\n",
    "    selectedCols = ['output', 'features'] + cols\n",
    "    t_data = t_data.select(selectedCols)\n",
    "        \n",
    "    return t_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_standard_scaler(t_data):\n",
    "    standardscaler=StandardScaler().setInputCol(\"features\").setOutputCol(\"scaled_features\")\n",
    "    t_data = standardscaler.fit(t_data).transform(t_data)\n",
    "    \n",
    "    return t_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "\n",
    "def train_test_split(t_data):\n",
    "    train, test = t_data.randomSplit([0.7, 0.3], seed = 2019)\n",
    "    print(\"Training Dataset Count: \" + str(train.count()))\n",
    "    print(\"Test Dataset Count: \" + str(test.count()))\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(t_data):\n",
    "    #Feature selection\n",
    "    css = ChiSqSelector(featuresCol='scaled_features',outputCol='Aspect',labelCol='output',fpr=0.05)\n",
    "    t_data=css.fit(t_data).transform(t_data)\n",
    "    \n",
    "    return t_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imbalance check - train only\n",
    "\n",
    "def get_balance_weight_ratio_data(t_data):\n",
    "    dataset_size=float(t_data.select(\"output\").count())\n",
    "    numPositives=t_data.select(\"output\").where('output == 1').count()\n",
    "    per_ones=(float(numPositives)/float(dataset_size))*100\n",
    "    numNegatives=float(dataset_size-numPositives)\n",
    "    \n",
    "    #Rebalance data\n",
    "    BalancingRatio = numNegatives/dataset_size\n",
    "    \n",
    "    return t_data.withColumn(\"classWeights\", when(t_data.output == 1,BalancingRatio).otherwise(1-BalancingRatio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_perf_summary(trainingSummary):\n",
    "    print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))\n",
    "    \n",
    "    accuracy = trainingSummary.accuracy\n",
    "    falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "    truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "    fMeasure = trainingSummary.weightedFMeasure()\n",
    "    precision = trainingSummary.weightedPrecision\n",
    "    recall = trainingSummary.weightedRecall\n",
    "    print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "          % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_perf_eval(predictions):\n",
    "    predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"probability\",\"output\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)\n",
    "    \n",
    "    evaluator = BinaryClassificationEvaluator(labelCol = 'output')\n",
    "    print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logistic_regression(tn_data):\n",
    "    lr = LogisticRegression(maxIter=10, featuresCol=\"Aspect\", labelCol=\"output\", \n",
    "                            weightCol=\"classWeights\", predictionCol=\"prediction\")\n",
    "    # Fit the model\n",
    "    lrModel = lr.fit(tn_data)\n",
    "\n",
    "    predict_train=lrModel.transform(tn_data)\n",
    "    #predict_test=lrModel.transform(ts_data)\n",
    "    \n",
    "    trainingSummary = lrModel.summary\n",
    "    \n",
    "    print_perf_summary(trainingSummary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random_forest_algorithm(tn_data, ts_data):\n",
    "    rf = RandomForestClassifier(numTrees=10, featuresCol=\"scaled_features\", labelCol=\"output\", predictionCol=\"prediction\")\n",
    "    rfModel = rf.fit(tn_data)\n",
    "    predictions = rfModel.transform(ts_data)\n",
    "    \n",
    "    print_perf_eval(predictions)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gradient_boost(tn_data, ts_data):\n",
    "    gbt = GBTClassifier(maxIter=10, featuresCol=\"scaled_features\", labelCol=\"output\", predictionCol=\"prediction\")\n",
    "    gbtModel = gbt.fit(tn_data)\n",
    "    predictions = gbtModel.transform(ts_data)\n",
    "    \n",
    "    print_perf_eval(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lsvc(tn_data, ts_data):\n",
    "    sv = LinearSVC(maxIter=10, regParam=0.1,\n",
    "                     featuresCol=\"scaled_features\", labelCol=\"output\", predictionCol=\"prediction\")\n",
    "    svModel = sv.fit(tn_data)\n",
    "    predictions = svModel.transform(ts_data)\n",
    "    \n",
    "    evaluator = BinaryClassificationEvaluator(labelCol = 'output')\n",
    "    print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill missing values\n",
    "df_train = fill_missing_val(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get columns and stages\n",
    "cols, num_cols, cat_cols = get_columns(df_train)\n",
    "stages = create_string_indexer_vector_assm_pipeline_stages(num_cols, cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 1075.804398 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "df_train = run_transformation_pipeline_stages(df_train, cols, stages)\n",
    "\n",
    "t1 = time.time()\n",
    "print('Runtime: %f seconds' % (float(t1 - t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3515.fit.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n\tat org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:342)\n\tat org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:335)\n\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:159)\n\tat org.apache.spark.SparkContext.clean(SparkContext.scala:2299)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(RDD.scala:850)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(RDD.scala:849)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.mapPartitionsWithIndex(RDD.scala:849)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:613)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:89)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-f62956e65f07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_standard_scaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-0493bd9cbd3b>\u001b[0m in \u001b[0;36mrun_standard_scaler\u001b[0;34m(t_data)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_standard_scaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mstandardscaler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetInputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetOutputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scaled_features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mt_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \"\"\"\n\u001b[1;32m    284\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o3515.fit.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n\tat org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:342)\n\tat org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:335)\n\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:159)\n\tat org.apache.spark.SparkContext.clean(SparkContext.scala:2299)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(RDD.scala:850)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(RDD.scala:849)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.mapPartitionsWithIndex(RDD.scala:849)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:613)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:89)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "df_train = run_standard_scaler(df_train)\n",
    "\n",
    "t1 = time.time()\n",
    "print('Runtime: %f seconds' % (float(t1 - t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "df_train = feature_selection(df_train)\n",
    "\n",
    "t1 = time.time()\n",
    "print('Runtime: %f seconds' % (float(t1 - t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "train, test = train_test_split(df_train)\n",
    "\n",
    "t1 = time.time()\n",
    "print('Runtime: %f seconds' % (float(t1 - t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "train = get_balance_weight_ratio_data(train)\n",
    "\n",
    "t1 = time.time()\n",
    "print('Runtime: %f seconds' % (float(t1 - t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "run_logistic_regression(train)\n",
    "\n",
    "t1 = time.time()\n",
    "print('Runtime: %f seconds' % (float(t1 - t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "run_random_forest_algorithm(train, test)\n",
    "\n",
    "t1 = time.time()\n",
    "print('Runtime: %f seconds' % (float(t1 - t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "run_gradient_boost(train, test)\n",
    "\n",
    "t1 = time.time()\n",
    "print('Runtime: %f seconds' % (float(t1 - t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "run_lsvc(train, test)\n",
    "\n",
    "t1 = time.time()\n",
    "print('Runtime: %f seconds' % (float(t1 - t0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}