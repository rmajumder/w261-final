{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Algorithm Theory\n",
    "\n",
    "Initial Table:\n",
    "\n",
    "<table>\n",
    "<th>Name</th>\n",
    "<th>Click</th>\n",
    "<th>Feature1</th>\n",
    "<th>Feature2</th>\n",
    "<tr><td>'firstad.png'</td><td>1</td><td>20</td><td>1</td></tr>\n",
    "<tr><td>'secondad.png'</td><td>1</td><td>10</td><td>5</td></tr>\n",
    "<tr><td>'firstad.png'</td><td>1</td><td>15</td><td>4</td></tr>\n",
    "<tr><td>'thirdad.png'</td><td>0</td><td>12</td><td>2</td></tr>\n",
    "<tr><td>'firstad.png'</td><td>0</td><td>30</td><td>1</td></tr>    \n",
    "</table>\n",
    "\n",
    "After Encoding:\n",
    "\n",
    "<table>\n",
    "<th>Name</th>\n",
    "<th>Click</th>\n",
    "<th>Feature1</th>\n",
    "<th>Feature2</th>\n",
    "<tr><td>0.0</td><td>1</td><td>20</td><td>1</td></tr>\n",
    "<tr><td>1.0</td><td>1</td><td>10</td><td>5</td></tr>\n",
    "<tr><td>0.0</td><td>1</td><td>15</td><td>4</td></tr>\n",
    "<tr><td>3.0</td><td>0</td><td>12</td><td>2</td></tr>\n",
    "<tr><td>0.0</td><td>0</td><td>30</td><td>1</td></tr>    \n",
    "</table>\n",
    "\n",
    "After Standard Scaler\n",
    "\n",
    "<table>\n",
    "<th>Name</th>\n",
    "<th>Click</th>\n",
    "<th>Feature1</th>\n",
    "<th>Feature2</th>\n",
    "<tr><td>0.0</td><td>1</td><td>2/3</td><td>1/5</td></tr>\n",
    "<tr><td>1.0</td><td>1</td><td>1/3</td><td>1</td></tr>\n",
    "<tr><td>0.0</td><td>1</td><td>1/2</td><td>4/5</td></tr>\n",
    "<tr><td>3.0</td><td>0</td><td>4/10</td><td>2/5</td></tr>\n",
    "<tr><td>0.0</td><td>0</td><td>1</td><td>1/5</td></tr>    \n",
    "</table>\n",
    "\n",
    "## Evaluation Function\n",
    "\n",
    "The evaluation function for logistic regression is:\n",
    "\n",
    "\n",
    "S(z) = $\\frac{1} {1 + e^{-z}}$\n",
    "\n",
    "where z is a linear combination of the features\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "The loss function for logistic regression is:\n",
    "\n",
    "Loss = $1/n*(-y^T*log(h)-(1-y)^T*log(1-h))$\n",
    "\n",
    "## Algorithm Demonstration\n",
    "\n",
    "\n",
    "## Iteration 1:\n",
    "\n",
    "\n",
    "\n",
    "## Part 3: Discussion of Challenges\n",
    "\n",
    "## EDA and train/test setup\n",
    "\n",
    "In our EDA, one of our considerations was whether to use the entire dataset or just a subset of data. On the one hand, we could obtain a complete overview of our samples by considering all samples. However, this could lead to longer times for our EDA (a little over 4 times as much time taking into account the increased communication time involved with partitioning more keys). We felt that it would be more prudent to consider 25% of the sample size for our EDA to make our EDA jobs run faster, taking into account that 25% of the sample size is still representative of the entire sample. \n",
    "\n",
    "Upon exploring the number, type, and distribution of columns, we saw that we had 14 numeric columns and 26 string columns. We initially explored on-hot encoding the string columns in our EDA. However, the expansion of columns with one-hot encoding was making the size of the data too large and Spark cluster was running out of memory In order to tackle this, we attempted to increase the memory allocated to the executer in spark to 128GB. However, this increase in memory resulted in slower speeds and was still not enough to tackle our memory issues. Finally, we used an alternate solution of encoding string columns as one numeric column with the StringIndexer method in the ml module in Spark. This helped us condense data in each string column into 1 column.\n",
    "\n",
    "Upon looking at the results of our EDA, we noticed that there are a lot of columns and some columns are fairly skewed even after using the standardscaler function in Spark. We decided to use a Chi Square Selected provided by the MlLib module in Spark, which selected columns based on independence, to select columns with the most predictive power.   \n",
    "\n",
    "## Model Selection\n",
    "\n",
    "We considered 4 different classification algorihtms in the mllib module of Spark for our algorithms. These were: Logistic Regression, Random Forest, Gradient Boosted Trees, and Linear Support Vector Classification. We did not try a Naive Bayes Classifier as some of the features were highly skewed and this could make training a Naive Bayes classifier problematic. We wanted to test the entire range of classification algorithms to test the best performing model based on the AUC metric of the ROC curve. \n",
    "\n",
    "We felt logistic regression was the simplest method to approach the classification problem. As it was simple, it would also be the one that is most efficient. However, logistic regression would not have the same power to address overfitting as some other algorithms such as Naive Bayes or Random Forests would.\n",
    "\n",
    "For Random Forest and Gradient Boosted Trees(GBT), we recognized that increasing the number of trees in GBT's can lead to an increase in training time and lead to overfitting. At the same time, we wished to keep training times for the Random Forest at a manageable range. This led us to decide on iterations of 10 and number of trees of 10 for GBTs and Random Forests respectively.\n",
    "\n",
    "For a Linear Support Vector Classifier, we felt it could offer a flexible model which may help counteract the skew in our data, however, we felt it could be lacking in revealing the reasons behind the classifications predicted by the model. \n",
    "\n",
    "With these considerations in mind, we went on to test our models on the classification task.\n",
    "\n",
    "## Part 5: Application of Course Concepts\n",
    "\n",
    "The following are some key course concepts we implemented in our approach:\n",
    "\n",
    "1. Subsampling-in order to decrease runtimes for our EDA, we used a subsample(25%) of our data, which we thought would still be representative of the entire dataset. This cut down on our EDA run time more than 4 times when taking into account the extra communication required to partition more keys in the entire sample versus the subsample.\n",
    "\n",
    "2. Dense Representation of Data to minimize memory usage-We condensed the string column data into a 1 column numeric representation rather than a one-hot encoded version which would comprise multiple columns. In doing this, we drew from lessons in the course on condensing data into dense representations in order to convey information making least use of memory to improve performance. This made the Spark jobs we ran faster, more scalable, and less likely to run out of memory.  \n",
    "\n",
    "3. Feature selection to improve predicting performance-we used a Chi Square Feature Selector to ensure that we selected the most independent and least correlated features to have maximum predictive power. This improved the scalability of our learning algorithms and made our model more predictive by removing extraneous features that were well correlated with other features.\n",
    "\n",
    "4. Normalization of features to improve model performance-so that the training could occur in a balanced and stable manner, we normalized our features using the StandardScaler function of the MlLib module in Spark. This made our algorithms more stable and improved our runtimes by cutting down on the chance of errors in algorithm runs.\n",
    "\n",
    "5. Use of ensemble methods to address overfitting-We used ensemble methods such as Random Forests and Gradient Boosted Trees to address overfitting. This made our implementation less prone to overfitting. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
